import asyncio
import os
import subprocess
import time
import socket
import random
import requests
import aioconsole
import atexit
import json
from logger_utils import setup_logger
from sentiment_utils import SentimentEngine, handle_level_change

# ğŸ›¡ï¸ å¯åŠ¨æ—¥å¿— & é˜²ä»£ç†
setup_logger()
os.environ["NO_PROXY"] = "127.0.0.1,localhost"

from config import (
    ACTIONS, SOVITS_ROOT, VTS_EXE_PATH, SOVITS_API_URL, VTS_PORT,
    INPUT_TIMEOUT, DEFAULT_BACKGROUND, SCENE_MAP
)
from vts_utils import VTSController
from audio_utils import AudioManager
from brain_utils import Brain
from memory_utils import MemoryManager
from sentiment_utils import SentimentEngine

# ================= âš™ï¸ å…¨å±€å˜é‡ =================
CURRENT_SPEAK_TASK = None
last_interaction_time = time.time()
global_memory_mgr = None


# ================= ğŸ“¨ è¾“å…¥ç¼“å†²ç®¡ç†å™¨ =================
class InputBufferManager:
    def __init__(self, timeout=1.5):
        self.buffer = []
        self.last_time = 0
        self.timeout = timeout
        self.is_processing = False

    def add_message(self, text):
        if not text.strip(): return
        self.buffer.append(text)
        self.last_time = time.time()
        print(f"ğŸ‘‚ (æ”¶åˆ°ç¢ç‰‡: {text}...)")

    def has_finished_speaking(self):
        if not self.buffer: return False
        if self.is_processing: return False
        return (time.time() - self.last_time) > self.timeout

    def pop_full_message(self):
        if not self.buffer: return None
        full_text = "ï¼Œ".join(self.buffer)
        self.buffer = []
        return full_text


# ================= ğŸ•µï¸â€â™‚ï¸ å·¥å…·å‡½æ•° =================
def is_process_running(process_name):
    try:
        return process_name in subprocess.getoutput(f'tasklist /FI "IMAGENAME eq {process_name}"')
    except:
        return False


def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) == 0


def launch_services():
    """ğŸš€ å¯åŠ¨æœåŠ¡ (å¸¦ç‹¬ç«‹è¿›ç¨‹æ£€æŸ¥)"""
    print("\n--------------- ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ•°å­—äººç¯å¢ƒ ---------------")

    # 1. æ£€æŸ¥å¹¶å¯åŠ¨ VTube Studio
    if is_process_running("VTube Studio.exe"):
        print("âœ… [æ£€æµ‹] VTube Studio å·²åœ¨è¿è¡Œï¼Œè·³è¿‡å¯åŠ¨ã€‚")
    elif os.path.exists(VTS_EXE_PATH):
        print("ğŸš€ [å¯åŠ¨] æ­£åœ¨å”¤é†’ VTube Studio...")
        subprocess.Popen(f'"{VTS_EXE_PATH}"', shell=True, close_fds=True)
        time.sleep(5)
    else:
        print("âš ï¸ [è­¦å‘Š] æ‰¾ä¸åˆ° VTube Studio è·¯å¾„ï¼Œè¯·æ‰‹åŠ¨å¯åŠ¨ã€‚")

    if not is_port_in_use(VTS_PORT):
        print("â³ ç­‰å¾… VTube Studio API å°±ç»ª...")
        for i in range(15):
            if is_port_in_use(VTS_PORT): break
            time.sleep(1)

    # 2. æ£€æŸ¥å¹¶å¯åŠ¨ GPT-SoVITS
    if is_port_in_use(9880):
        print("âœ… [æ£€æµ‹] GPT-SoVITS æœåŠ¡å·²åœ¨çº¿ï¼Œè·³è¿‡å¯åŠ¨ã€‚")
    else:
        print("ğŸš€ [å¯åŠ¨] æ­£åœ¨å¯åŠ¨è¯­éŸ³æœåŠ¡ (GPT-SoVITS)...")
        python_exe = os.path.join(SOVITS_ROOT, "runtime", "python.exe")
        try:
            cmd = f'start /min "" "{python_exe}" api_v2.py -c fufu.yaml'
            subprocess.Popen(cmd, shell=True, cwd=SOVITS_ROOT)
            print("â³ ç­‰å¾… TTS æœåŠ¡åŠ è½½ (çº¦8ç§’)...")
            time.sleep(8)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] æ— æ³•å¯åŠ¨ SoVITS: {e}")

    try:
        requests.post(f"{SOVITS_API_URL}/tts", json={"text": "ã€‚", "text_lang": "zh", "ref_audio_path": "dummy.wav"},
                      timeout=5)
    except:
        pass
    print("--------------- ç¯å¢ƒæ£€æŸ¥å®Œæ¯• ---------------")


# ğŸ”¥ æ‰“å°çŠ¶æ€æ¡ (æ˜¾ç¤ºå½“å‰æ´»åŠ¨)
def print_status_prompt(username, memory_mgr, sentiment_engine):
    user_state = memory_mgr.get_user_state_obj()
    global_state = sentiment_engine.get_global_state()

    mood = global_state["mood"]
    energy = global_state["energy"]
    # ğŸ”¥ è¿™é‡Œå°±æ˜¯ä½ åœ¨ global_state.json é‡Œçœ‹åˆ°çš„ "current_activity"
    current_act = global_state.get("current_activity", "æœªçŸ¥")

    if len(current_act) > 10: current_act = current_act[:9] + "..."

    mood_icon = "ğŸ˜"
    if mood >= 80:
        mood_icon = "ğŸ˜†"
    elif mood <= 20:
        mood_icon = "ğŸ˜¡"
    elif mood <= 40:
        mood_icon = "ğŸ˜"

    energy_icon = "âš¡"
    if energy < 30: energy_icon = "ğŸª«"

    status_bar = f"\n[ğŸ’– {int(user_state.affection)} | ğŸ“… {current_act} | {energy_icon} {int(energy)} | {mood_icon} {int(mood)}] ğŸ‘¤ {username}: "
    print(status_bar, end="", flush=True)


def emergency_save():
    if global_memory_mgr:
        print("\nğŸš¨ [ç´§æ€¥å­˜æ¡£] æ£€æµ‹åˆ°ç¨‹åºå¼‚å¸¸ä¸­æ–­ï¼Œæ­£åœ¨å°è¯•å¼ºåˆ¶ä¿å­˜...")
        global_memory_mgr.save()
        print("âœ… [ç´§æ€¥å­˜æ¡£] æ•°æ®å·²å†™å›ç£ç›˜ã€‚")


atexit.register(emergency_save)


# [main.py]
# [main.py] ä¸­çš„ monitor_idle_status å‡½æ•°
async def monitor_idle_status(vts, audio_mgr, brain, memory_mgr, sentiment_engine, input_mgr):
    global last_interaction_time, CURRENT_SPEAK_TASK

    IDLE_START_TIME = 30
    PROACTIVE_TALK_THRESHOLD = 120  # 120ç§’ä¸è¯´è¯è§¦å‘å¸¸è§„æ­è¯

    last_idle_action_time = 0
    has_triggered_talk = False
    idle_talk_sequence = 0

    last_detected_activity = sentiment_engine.get_global_state().get("current_activity", "")

    while True:
        await asyncio.sleep(1)

        # 1. å¿™ç¢Œè·³è¿‡
        if input_mgr.is_processing:
            last_interaction_time = time.time()
            continue

        # ================= ç”Ÿæ´»æµè‡ªåŠ¨åˆ‡æ¢ =================
        is_switched, new_act, new_loc = sentiment_engine.attempt_auto_switch(last_interaction_time)
        if is_switched:
            for key, bg_file in SCENE_MAP.items():
                if key in new_act:
                    await vts.set_background(bg_file)
                    break
        # =================================================

        # 2. è·å–å½“å‰çŠ¶æ€
        user_state = memory_mgr.get_user_state_obj()
        g_state = sentiment_engine.get_global_state()
        current_energy = g_state["energy"]
        current_act = g_state.get("current_activity", "")  # å†æ¬¡ç¡®è®¤æœ€æ–°çŠ¶æ€

        # 3. æä½å¥½æ„Ÿ/ç²¾åŠ›ä¸è¯´è¯
        if user_state.affection <= -50 or current_energy < 10:
            last_detected_activity = current_act
            continue

        # 4. æ­£åœ¨è¯´è¯è·³è¿‡
        if CURRENT_SPEAK_TASK and not CURRENT_SPEAK_TASK.done():
            last_interaction_time = time.time()
            continue

        # 5. è®¡ç®—æ²‰é»˜æ—¶é—´
        now = time.time()
        idle_duration = now - last_interaction_time

        # é‡ç½®é€»è¾‘
        if idle_duration < 2:
            idle_talk_sequence = 0
            has_triggered_talk = False
            last_detected_activity = current_act

        # 6. é—²ç½®åŠ¨ä½œ (VTS)
        if idle_duration > IDLE_START_TIME and (now - last_idle_action_time) > 15:
            if current_energy > 20:
                safe_actions = list(ACTIONS.keys())
                if safe_actions:
                    await vts.trigger_action(random.choice(safe_actions))
            last_idle_action_time = now

        # 7. ğŸ”¥ ä¸»åŠ¨æ­è¯é€»è¾‘ (é˜²å¤è¯» + æŠ¥å¤‡å¼) ğŸ”¥
        should_talk = is_switched or (idle_duration > PROACTIVE_TALK_THRESHOLD and not has_triggered_talk)

        if should_talk:
            if current_energy >= 30:
                idle_talk_sequence += 1

                # A. æ„é€ è§¦å‘è¯­å¢ƒ
                if is_switched:
                    user_text_simulated = f"(èŠ™å®å¨œçš„ç”Ÿæ´»æµæ¨è¿›ï¼šä»ã€{last_detected_activity}ã€‘åˆ‡æ¢åˆ°äº†ã€{current_act}ã€‘)"
                    # åˆ‡æ¢åœºæ™¯æ—¶ï¼šç¨å¾®ç½—å—¦ä¸€ç‚¹ï¼Œäº¤ä»£å‰å› åæœ
                    injection = f"""
                    âš ï¸ã€ç”Ÿæ´»æµè½¬åœºã€‘ä½ åˆšåˆšç»“æŸäº†ä¸Šä¸€é¡¹æ´»åŠ¨ï¼Œç°åœ¨å¼€å§‹äº†æ–°æ´»åŠ¨ã€‚
                    è¯·ç”¨è‡ªè¨€è‡ªè¯­çš„æ–¹å¼ï¼š
                    1. ç®€å•æŠ±æ€¨æˆ–æ„Ÿå¹ä¸€ä¸‹ä¸Šä¸€ä»¶äº‹ï¼ˆæ¯”å¦‚â€œç»ˆäºåšå®Œäº†â€ï¼‰ã€‚
                    2. æè¿°ç°åœ¨è¦åšçš„äº‹ã€‚
                    """
                    print(f"ğŸ¬ [ç”Ÿæ´»æµ] çŠ¶æ€åˆ‡æ¢: {last_detected_activity} -> {current_act}")
                else:
                    # é—²ç½®æ­è¯ï¼šçº¯æŠ¥å¤‡ï¼Œä¸å¼ºæ±‚å¯¹è¯
                    user_text_simulated = "(ç”¨æˆ·åœ¨å¿™ï¼ŒèŠ™å®å¨œè‡ªå·±åœ¨åšè‡ªå·±çš„äº‹)"

                    # ğŸ”¥ ä¿®å¤2ï¼šåŠ å…¥éšæœºåˆ‡å…¥ç‚¹ï¼Œé˜²æ­¢å¤§è„‘æ­»å¾ªç¯ç”ŸæˆåŒä¸€å¥è¯
                    focus_points = [
                        "æŠ±æ€¨æŸä¸ªå…·ä½“çš„ç»†èŠ‚",
                        "å¯¹ç¯å¢ƒçš„æ„Ÿå®˜æå†™ï¼ˆå£°éŸ³/æ°”å‘³/å…‰çº¿ï¼‰",
                        "çªç„¶æƒ³èµ·çš„ä¸€ä»¶å¾€äº‹",
                        "å¯¹è‡ªå·±èº«ä½“çŠ¶æ€çš„æ„Ÿå—ï¼ˆç´¯/é¥¿/å›°ï¼‰",
                        "å¯¹æœªæ¥çš„ä¸€ä¸ªå°æœŸå¾…",
                        "å“¼ä¸€æ®µæ—‹å¾‹æˆ–æ‹Ÿå£°è¯"
                    ]
                    current_focus = random.choice(focus_points)

                    injection = f"""
                                        âš ï¸ã€é—²ç½®æŠ¥å¤‡ã€‘ç”¨æˆ·ç°åœ¨æ²¡ç©ºç†ä½ ã€‚
                                        **ä¸è¦å‘ç”¨æˆ·æé—®ï¼ä¸è¦è¯•å›¾å¼€å¯æ–°è¯é¢˜ï¼**
                                        ä½ æ­£åœ¨ã€{current_act}ã€‘ã€‚

                                        è¯·**å¿…é¡»**ä»è¿™ä¸ªè§’åº¦åˆ‡å…¥ï¼šã€{current_focus}ã€‘ã€‚

                                        è¦æ±‚ï¼šç®€çŸ­åœ°å˜Ÿå›”ä¸€å¥ï¼ˆ20å­—ä»¥å†…ï¼‰ã€‚
                                        ä¸è¦é‡å¤ä¹‹å‰è¯´è¿‡çš„è¯ï¼
                                        """
                    print(f"ğŸ‘€ [è§‚å¯Ÿ] ç”¨æˆ·æ²‰é»˜ï¼Œå°è¯•ç¬¬ {idle_talk_sequence} æ¬¡è‡ªè¨€è‡ªè¯­ (åˆ‡å…¥ç‚¹: {current_focus})...")

                # B. å‡†å¤‡æ•°æ®
                current_snapshot = {
                    "location": g_state.get("current_location", "å§å®¤"),
                    "activity": current_act,  # ç¡®ä¿ä¼ ç»™å¤§è„‘çš„æ˜¯æœ€æ–°æ´»åŠ¨
                    "item": g_state.get("current_item", "æ— "),
                    "energy": g_state["energy"],
                    "mood": g_state["mood"]
                }

                history_str = memory_mgr.get_formatted_history(limit=10)
                try:
                    last_chat_info_str = memory_mgr.get_last_chat_info()

                    decision_result = brain.unified_decision_maker(
                        user_text=user_text_simulated,
                        current_state_dict=current_snapshot,
                        sentiment_injection=injection,
                        history_str=history_str,
                        memory_long_term=memory_mgr.data.get("summary", ""),
                        memory_global=memory_mgr.get_global_activity_log(limit=3),  # è®°å¾—è¿™é‡Œä¹Ÿè¦è·Ÿè¿›ä¹‹å‰çš„ä¿®æ”¹
                        relationship_info=f"- åå­—: {memory_mgr.current_user}",
                        social_context="æš‚æ— ",
                        last_chat_info=last_chat_info_str
                    )

                    final_text = decision_result["reply_text"]

                    # ğŸ”¥ğŸ”¥ğŸ”¥ é˜²å¤è¯»æ£€æµ‹æ ¸å¿ƒé€»è¾‘ ğŸ”¥ğŸ”¥ğŸ”¥
                    # 1. æ£€æŸ¥æœ€è¿‘ 5 æ¡è®°å½•
                    recent_history = memory_mgr.get_recent_history(limit=5)
                    is_duplicate = False
                    for h in recent_history:
                        # å¦‚æœæ˜¯èŠ™å®å¨œè¯´çš„ï¼Œä¸”å†…å®¹åŒ…å«ç°åœ¨çš„å›å¤
                        if h["role"] == "assistant" and final_text in h["content"]:
                            is_duplicate = True
                            break

                    # 2. åˆ†æƒ…å†µå¤„ç†
                    if is_duplicate:
                        print(f"ğŸ”‡ [ç³»ç»Ÿ] æ£€æµ‹åˆ°å¤è¯»æœºè¡Œä¸ºï¼Œå·²æ‹¦æˆª: {final_text}")
                        # âš ï¸ å…³é”®ç­–ç•¥ï¼šè¢«æ‹¦æˆªåï¼Œåªå›é€€ä¸€ç‚¹æ—¶é—´ï¼ˆä¾‹å¦‚è®©å®ƒä»¥ä¸ºå·²ç»è¿‡äº†110ç§’ï¼‰
                        # è¿™æ · 10 ç§’å (120 - 110 = 10) å¥¹å°±ä¼šå†æ¬¡å°è¯•æ­è¯ï¼Œè€Œä¸æ˜¯é‡æ–°ç­‰ 120 ç§’
                        last_interaction_time = time.time() - (PROACTIVE_TALK_THRESHOLD - 10)

                    else:
                        # âœ… æ­£å¸¸æ’­æ”¾é€»è¾‘
                        new_user_state, _ = sentiment_engine.apply_decision_and_update(
                            "æ— ", user_state, decision_result
                        )
                        memory_mgr.save_user_state(new_user_state)

                        print(f"\nğŸ­ èŠ™å®å¨œ(ä¸»åŠ¨): {final_text}")
                        memory_mgr.add_history("assistant", final_text)
                        memory_mgr.save()
                        CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(final_text, vts))
                        print_status_prompt(memory_mgr.current_user, memory_mgr, sentiment_engine)

                        # âš ï¸ åªæœ‰æˆåŠŸè¯´è¯äº†ï¼Œæ‰å½»åº•é‡ç½®è®¡æ—¶å™¨å’ŒçŠ¶æ€ï¼
                        last_interaction_time = time.time()
                        has_triggered_talk = True
                        last_detected_activity = current_act

                except Exception as e:
                    print(f"âš ï¸ ä¸»åŠ¨æ­è¯å¤±è´¥: {e}")
                    # å‡ºé”™ä¹Ÿæš‚æ—¶é‡ç½®ï¼Œé˜²æ­¢æ­»å¾ªç¯åˆ·æŠ¥é”™
                    last_interaction_time = time.time()

async def listen_loop(input_mgr, username):
    print("ğŸ¤ [ç³»ç»Ÿ] ç›‘å¬æœåŠ¡å·²å¯åŠ¨ (è¾“å…¥ 'exit' é€€å‡º)...")
    while True:
        try:
            text = await aioconsole.ainput("")
            input_mgr.add_message(text)
        except asyncio.CancelledError:
            break
        except Exception as e:
            await asyncio.sleep(1)


# ================= ğŸ¬ ä¸»ç¨‹åº =================
async def main():
    global CURRENT_SPEAK_TASK, last_interaction_time, global_memory_mgr

    print("\nğŸ“š === èŠ™å®å¨œçš„è®°å¿†æ®¿å ‚ ===")
    username = input("è¯·è¾“å…¥ä½ çš„åå­— (è¯»å–å­˜æ¡£): ").strip()
    if not username: username = "æ—…è¡Œè€…"

    launch_services()

    vts = VTSController(port=VTS_PORT)
    audio_mgr = AudioManager()
    # âŒ åˆ é™¤äº† BGMManagerï¼Œé˜²æ­¢æŠ¥é”™

    brain = Brain()
    memory_mgr = MemoryManager()
    global_memory_mgr = memory_mgr
    sentiment_engine = SentimentEngine()
    input_mgr = InputBufferManager(timeout=INPUT_TIMEOUT)

    memory_mgr.load_user(username)
    user_state = memory_mgr.get_user_state_obj()
    g_state = sentiment_engine.get_global_state()

    if not await vts.connect(): return

    lvl, title, _ = memory_mgr.calculate_status()
    print(f"\nğŸ‰ === èŠ™å®å¨œ & {username} ===")
    print(f"ğŸ’– å¥½æ„Ÿåº¦: {user_state.affection} | ğŸ­ ç­‰çº§: {title}")
    print(f"âš¡ ç²¾åŠ›å€¼: {int(g_state['energy'])}/100 | ğŸ˜Š å¿ƒæƒ…: {int(g_state['mood'])}/100")
    print("==============================\n")

    chat_history = memory_mgr.data.get('chat_history', [])
    summary = memory_mgr.data.get("summary", "")
    is_new_user = (len(chat_history) == 0) and (not summary)

    welcome = ""
    if is_new_user:
        welcome = f"[å‚²å¨‡] å’³å’³ï¼åˆæ¬¡è§é¢ï¼Œ{username}ï¼æˆ‘æ˜¯èŠ™å®å¨œÂ·å¾·Â·æ«ä¸¹ï¼"
    elif user_state.affection <= -100:
        welcome = "[ç”Ÿæ°”] â€¦â€¦ï¼ˆæ— è§†ï¼‰"
    else:
        welcome = f"[å‚²å¨‡] åˆæ˜¯ä½ å•Šï¼Œ{username}ã€‚"
        if user_state.affection >= 300:
            print("ğŸ¤” (èŠ™èŠ™æ­£åœ¨å›å¿†ä¸Šæ¬¡èŠäº†ä»€ä¹ˆ...)")
            dynamic_welcome = brain.generate_dynamic_welcome(
                memory_mgr, g_state['mood'], g_state['energy'], g_state.get('current_activity', 'å‘å‘†'),
                g_state.get('current_location', 'å®¶é‡Œ')
            )
            if dynamic_welcome: welcome = dynamic_welcome

    print(f"ğŸ­ èŠ™å®å¨œ: {welcome}")
    if user_state.affection > -100:
        memory_mgr.add_history("assistant", welcome)

    CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(welcome, vts))

    # ğŸ”¥ğŸ”¥ğŸ”¥ ä¿®å¤ç‚¹ï¼šè¿™é‡Œå½»åº•åˆ é™¤äº† bgm_mgr å‚æ•°ï¼Œè§£å†³ä½ çš„æˆªå›¾æŠ¥é”™ ğŸ”¥ğŸ”¥ğŸ”¥
    monitor_task = asyncio.create_task(
        monitor_idle_status(vts, audio_mgr, brain, memory_mgr, sentiment_engine, input_mgr))

    listen_task = asyncio.create_task(listen_loop(input_mgr, username))
    print_status_prompt(username, memory_mgr, sentiment_engine)

    try:
        while True:
            await asyncio.sleep(0.1)
            memory_mgr.data["last_interaction_timestamp"] = time.time()

            if input_mgr.has_finished_speaking():
                user_input = input_mgr.pop_full_message()

                # --- ğŸ†• æ–°å¢ï¼šæ‰“æ–­æ£€æµ‹é€»è¾‘ (è¡¥å…¨æˆªå›¾åŠŸèƒ½) ---
                if CURRENT_SPEAK_TASK and not CURRENT_SPEAK_TASK.done():
                    print("ğŸ›‘ [ç³»ç»Ÿ] æ£€æµ‹åˆ°ç”¨æˆ·æ‰“æ–­ï¼")

                    # 1. ç‰©ç†æ‰“æ–­ï¼šåœæ­¢å½“å‰è¯­éŸ³
                    audio_mgr.stop()
                    CURRENT_SPEAK_TASK.cancel()

                    # 2. æƒ…æ„Ÿååº”ï¼šè·å–ç”Ÿæ°”/è¢«æ‰“æ–­çš„ååº”
                    # check_blacklist_state æ˜¯é»‘åå•ï¼Œè¿™é‡Œåº”è¯¥ç”¨ get_interruption_reaction
                    mood_penalty, emo_icon, anger_reply = sentiment_engine.get_interruption_reaction()

                    # 3. è¾“å‡ºååº”
                    print(f"{emo_icon} èŠ™å®å¨œ(è¢«æ‰“æ–­): {anger_reply}")
                    memory_mgr.add_history("assistant", anger_reply)  # å†™å…¥è®°å¿†ï¼Œè®©å¥¹è®°å¾—è‡ªå·±ç”Ÿæ°”äº†

                    # 4. ç«‹å³æ’­æ”¾ç”Ÿæ°”çš„è¯­éŸ³
                    CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(anger_reply, vts))

                    # 5. æ‰“å°çŠ¶æ€æ¡å¹¶è·³è¿‡æœ¬æ¬¡æ­£å¸¸çš„ AI æ€è€ƒ
                    print_status_prompt(username, memory_mgr, sentiment_engine)
                    last_interaction_time = time.time()
                    input_mgr.is_processing = False
                    continue  # <--- è·³è¿‡åç»­çš„ deepseek æ€è€ƒï¼Œç›´æ¥è¿›å…¥ä¸‹ä¸€è½®å¾ªç¯
                # ---------------------------------------------

                if user_input.lower() in ["quit", "exit", "é€€å‡º", "å†è§", "æ‹œæ‹œ"]:
                    print("\nğŸ’¾ [ç³»ç»Ÿ] æ­£åœ¨æ•´ç†è®°å¿†å¹¶ç”Ÿæˆæ—¥è®°ï¼Œè¯·ç¨å€™...")
                    # è¿™é‡Œçš„ brain å‚æ•°æ˜¯ä¸»ç¨‹åºé‡Œåˆå§‹åŒ–çš„é‚£ä¸ª brain å¯¹è±¡
                    memory_mgr.archive_session(brain)
                    break

                input_mgr.is_processing = True
                last_interaction_time = time.time()
                print(f"\nğŸ“ [ç”¨æˆ·] {user_input}")

                # ============================================
                # ğŸš€ V35.0 æ ¸å¿ƒï¼šæ³¨å…¥çŠ¶æ€ + ç»Ÿä¸€å†³ç­– (ä¼˜åŒ–å»¶è¿Ÿ)
                # ============================================

                # âœ…âœ…âœ… å…³é”®ä¿®å¤ï¼šåœ¨è¿™é‡Œå…ˆè·å– user_stateï¼Œé˜²æ­¢ UnboundLocalError âœ…âœ…âœ…
                current_user_state = memory_mgr.get_user_state_obj()

                # 1. å‡†å¤‡æ‰€æœ‰çŠ¶æ€ (å¿ƒæƒ…ã€åœ°ç‚¹ã€ç²¾åŠ›)
                g_state = sentiment_engine.get_global_state()
                current_snapshot = {
                    "location": g_state.get("current_location", "å§å®¤"),
                    "activity": g_state.get("current_activity", "å‘å‘†"),
                    "item": g_state.get("current_item", "æ— "),
                    "energy": g_state["energy"],
                    "mood": g_state["mood"]
                }

                history_str = memory_mgr.get_formatted_history(limit=20)
                memory_long_term = memory_mgr.data.get("summary", "æš‚æ— ç‰¹æ®Šå›å¿†")
                memory_global = memory_mgr.get_global_activity_log(limit=5)

                # ğŸ”¥ğŸ”¥ğŸ”¥ V35.2 æ–°å¢ï¼šæå–å…³ç³»ä¸å…«å¦æ•°æ® ğŸ”¥ğŸ”¥ğŸ”¥
                # A. è·å–å¥½æ„Ÿåº¦æè¿° (æ ‡é¢˜+åŸºç¡€æ€åº¦)
                rel_title, rel_base_desc = memory_mgr.get_relationship_base_desc()
                user_aff = current_user_state.affection  # âœ… ç°åœ¨è¿™é‡Œå¯ä»¥æ­£å¸¸è¿è¡Œäº†

                # ç»„è£…å…³ç³»å­—ç¬¦ä¸²
                relationship_info_str = f"""
                                - åå­—: {username}
                                - å¥½æ„Ÿåº¦: {user_aff}
                                - ç­‰çº§: ã€{rel_title}ã€‘
                                - åŸºç¡€æ€åº¦: {rel_base_desc}
                                """

                # B. è·å–å…³äºè¯¥ç”¨æˆ·çš„ç¤¾äº¤å…«å¦
                social_context_str = memory_mgr.get_social_context(username)

                related_memories_str = ""
                # 1. è·å–æ‰€æœ‰è®¤è¯†çš„äººçš„åå•
                all_contacts = memory_mgr.get_known_users()
                found_contacts = []

                # 2. éå†åå•ï¼Œçœ‹ç”¨æˆ·è¿™å¥â€œè¯â€é‡Œæœ‰æ²¡æœ‰æåˆ°è°
                for name in all_contacts:
                    # æ’é™¤è‡ªå·±ï¼ŒåªæŸ¥åˆ«äºº
                    if name in user_input and name != username:
                        memo = memory_mgr.get_person_brief(name)
                        if memo:
                            found_contacts.append(memo)
                            print(f"ğŸ” [è”æƒ³] èŠ™èŠ™æƒ³èµ·äº†: {name}")

                if found_contacts:
                    related_memories_str = "ä½ å¿½ç„¶æƒ³èµ·äº†å…³äºè¿™äº›äººçš„è®°å¿†ï¼š\n" + "\n".join(found_contacts)
                else:
                    related_memories_str = "ï¼ˆè¯è¯­ä¸­æœªæåŠå…¶ä»–ç†Ÿäººï¼‰"

                    # âœ…ã€ä¿®æ­£åã€‘é€»è¾‘è·³å‡ºäº† elseï¼Œæ— è®ºæœ‰æ²¡æœ‰æåˆ°äººï¼Œéƒ½ä¼šæ‰§è¡Œä¸‹é¢çš„æ€è€ƒé€»è¾‘

                    # ğŸ”¥ğŸ”¥ğŸ”¥ V37.0 æ–°å¢ï¼šè·å–ä¸Šæ¬¡èŠå¤©æƒ…æŠ¥ ğŸ”¥ğŸ”¥ğŸ”¥
                last_chat_info_str = memory_mgr.get_last_chat_info()
                print(f"â° [è®°å¿†] ä¸Šæ¬¡äº’åŠ¨: {last_chat_info_str.replace(chr(10), ' | ')}")

                rag_memories_str = ""
                if len(user_input) > 2:
                    print("ğŸ”¦ [è®°å¿†] æ­£åœ¨ç¿»é˜…æ—§æ—¥è®°...")
                    rag_memories_str = memory_mgr.search_relevant_memories(user_input)
                    if not rag_memories_str:
                        rag_memories_str = "(æœªæ‰¾åˆ°ç›¸å…³å¾€äº‹)"

                # 3. ğŸ§  è°ƒç”¨ç»Ÿä¸€å†³ç­–æœº
                print("â³ (èŠ™å®å¨œæ­£åœ¨æ€è€ƒä¸è¡ŒåŠ¨...)")
                decision_result = brain.unified_decision_maker(
                    user_text=user_input,
                    current_state_dict=current_snapshot,
                    sentiment_injection="",
                    history_str=history_str,
                    memory_long_term=memory_long_term,
                    memory_global=memory_global,
                    relationship_info=relationship_info_str,
                    social_context=social_context_str,
                    related_memories=related_memories_str,
                    last_chat_info=last_chat_info_str,
                    rag_context=rag_memories_str
                )

                final_text = decision_result["reply_text"]
                print(
                    f"ğŸ§  [çŠ¶æ€] {decision_result['next_state']['activity']} @ {decision_result['next_state']['location']}")

                # 4. åå¤„ç†ï¼šæ›´æ–°æ•°å€¼
                # [A] è®°å½•æ—§çš„å¥½æ„Ÿåº¦
                old_affection = current_user_state.affection

                # [B] æ‰§è¡Œæ›´æ–°
                new_user_state, current_act = sentiment_engine.apply_decision_and_update(
                    user_input,
                    current_user_state,
                    decision_result
                )

                # [C] è®°å½•æ–°çš„å¥½æ„Ÿåº¦
                new_affection = new_user_state.affection

                # [D] å¼ºåˆ¶æ›´æ–°æœ€åæ´»è·ƒæ—¶é—´
                import datetime
                new_user_state.last_active_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")

                memory_mgr.save_user_state(new_user_state)

                # 5. è¾“å‡º & æ’­æ”¾
                memory_mgr.add_history("assistant", final_text)
                memory_mgr.save()
                print(f"\rğŸ­ èŠ™å®å¨œ: {final_text}")

                CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(final_text, vts))

                # ğŸ”¥ğŸ”¥ğŸ”¥ [æ ¸å¿ƒæ’å…¥ç‚¹] æ£€æŸ¥ç­‰çº§å˜åŒ– ğŸ”¥ğŸ”¥ğŸ”¥
                # å¿…é¡»æ”¾åœ¨ speak ä¹‹åï¼Œåˆ©ç”¨å¼‚æ­¥ä»»åŠ¡å»æ£€æŸ¥ï¼Œä¸å¡é¡¿ä¸»æµç¨‹
                asyncio.create_task(handle_level_change(
                    vts, audio_mgr, brain, memory_mgr,
                    username,  # ä¼ å…¥å½“å‰çš„ç”¨æˆ·å
                    old_affection,  # æ—§å¥½æ„Ÿ
                    new_affection  # æ–°å¥½æ„Ÿ
                ))

                # 6. å…«å¦æå– (å¼‚æ­¥ä¸é˜»å¡)
                known_users = memory_mgr.get_known_users()
                gossip, gender_list = brain.extract_social_gossip(user_input, username, known_users)
                if gossip:
                    t, r, c = gossip
                    memory_mgr.update_social_relation(username, t, r, c)
                    print(f"ğŸ•¸ï¸ [å…«å¦] è®°ä½äº† {username} {r} {t}")

                if "å¸¦å›å»" in user_input or "æ”¶å…»" in user_input:
                    fact = brain.extract_important_fact(f"èŠ™å®å¨œå†³å®šï¼š{final_text}", username)
                    if fact:
                        print(f"ğŸ“ [è®°å¿†] è®°å½•é‡è¦äº‹å®: {fact}")
                        # å¼ºåˆ¶è¿½åŠ åˆ° summary é‡Œï¼Œè¿™æ ·å¥¹æ°¸è¿œä¸ä¼šå¿˜ï¼
                        memory_mgr.data["summary"] += f"\n- {fact} ({time.strftime('%Y-%m-%d')})"
                        memory_mgr.save()
                last_interaction_time = time.time()
                input_mgr.is_processing = False
                print_status_prompt(username, memory_mgr, sentiment_engine)

    except KeyboardInterrupt:
        print("\nğŸ›‘ å¼ºåˆ¶é€€å‡º...")
    finally:
        if vts: await vts.close()
        if hasattr(memory_mgr, "save"): memory_mgr.save()
        print("ğŸ‘‹ ç¨‹åºå·²å…³é—­")


if __name__ == "__main__":
    asyncio.run(main())