import asyncio
import os
import subprocess
import time
import socket
import random
import requests
import aioconsole
import atexit
from logger_utils import setup_logger

# ğŸ›¡ï¸ å¯åŠ¨æ—¥å¿— & é˜²ä»£ç†
setup_logger()
os.environ["NO_PROXY"] = "127.0.0.1,localhost"

from config import (
    ACTIONS, SOVITS_ROOT, VTS_EXE_PATH, SOVITS_API_URL, VTS_PORT,
    INPUT_TIMEOUT, DEFAULT_BACKGROUND, SCENE_MAP
)
from vts_utils import VTSController
# âœ… å¼•å…¥ BGMManager
from audio_utils import AudioManager, BGMManager
from brain_utils import Brain
from memory_utils import MemoryManager
from sentiment_utils import SentimentEngine

# ================= âš™ï¸ å…¨å±€å˜é‡ =================
CURRENT_SPEAK_TASK = None
last_interaction_time = time.time()
global_memory_mgr = None


# ================= ğŸ“¨ è¾“å…¥ç¼“å†²ç®¡ç†å™¨ =================
class InputBufferManager:
    def __init__(self, timeout=1.5):
        self.buffer = []
        self.last_time = 0
        self.timeout = timeout
        self.is_processing = False

    def add_message(self, text):
        if not text.strip(): return
        self.buffer.append(text)
        self.last_time = time.time()
        print(f"ğŸ‘‚ (æ”¶åˆ°ç¢ç‰‡: {text}...)")

    def has_finished_speaking(self):
        if not self.buffer: return False
        if self.is_processing: return False
        return (time.time() - self.last_time) > self.timeout

    def pop_full_message(self):
        if not self.buffer: return None
        full_text = "ï¼Œ".join(self.buffer)
        self.buffer = []
        return full_text


# ================= ğŸ•µï¸â€â™‚ï¸ å·¥å…·å‡½æ•° =================
def is_process_running(process_name):
    try:
        return process_name in subprocess.getoutput(f'tasklist /FI "IMAGENAME eq {process_name}"')
    except:
        return False


def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) == 0


def launch_services():
    """
    ğŸš€ å¯åŠ¨æœåŠ¡ (å¸¦ç‹¬ç«‹è¿›ç¨‹æ£€æŸ¥)
    """
    print("\n--------------- ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ•°å­—äººç¯å¢ƒ ---------------")

    # 1. æ£€æŸ¥å¹¶å¯åŠ¨ VTube Studio
    if is_process_running("VTube Studio.exe"):
        print("âœ… [æ£€æµ‹] VTube Studio å·²åœ¨è¿è¡Œï¼Œè·³è¿‡å¯åŠ¨ã€‚")
    elif os.path.exists(VTS_EXE_PATH):
        print("ğŸš€ [å¯åŠ¨] æ­£åœ¨å”¤é†’ VTube Studio...")
        subprocess.Popen(f'"{VTS_EXE_PATH}"', shell=True, close_fds=True)
        time.sleep(5)
    else:
        print("âš ï¸ [è­¦å‘Š] æ‰¾ä¸åˆ° VTube Studio è·¯å¾„ï¼Œè¯·æ‰‹åŠ¨å¯åŠ¨ã€‚")

    # ç­‰å¾… VTS ç«¯å£å°±ç»ª
    if not is_port_in_use(VTS_PORT):
        print("â³ ç­‰å¾… VTube Studio API å°±ç»ª...")
        for i in range(15):
            if is_port_in_use(VTS_PORT): break
            time.sleep(1)

    # 2. æ£€æŸ¥å¹¶å¯åŠ¨ GPT-SoVITS
    if is_port_in_use(9880):
        print("âœ… [æ£€æµ‹] GPT-SoVITS æœåŠ¡å·²åœ¨çº¿ï¼Œè·³è¿‡å¯åŠ¨ã€‚")
    else:
        print("ğŸš€ [å¯åŠ¨] æ­£åœ¨å¯åŠ¨è¯­éŸ³æœåŠ¡ (GPT-SoVITS)...")
        python_exe = os.path.join(SOVITS_ROOT, "runtime", "python.exe")
        try:
            cmd = f'start /min "" "{python_exe}" api_v2.py -c fufu.yaml'
            subprocess.Popen(cmd, shell=True, cwd=SOVITS_ROOT)

            print("â³ ç­‰å¾… TTS æœåŠ¡åŠ è½½ (çº¦8ç§’)...")
            time.sleep(8)
        except Exception as e:
            print(f"âŒ [é”™è¯¯] æ— æ³•å¯åŠ¨ SoVITS: {e}")

    # 3. æœ€åçš„è¿é€šæ€§æµ‹è¯• (é¢„çƒ­)
    try:
        requests.post(f"{SOVITS_API_URL}/tts", json={"text": "ã€‚", "text_lang": "zh", "ref_audio_path": "dummy.wav"},
                      timeout=5)
    except:
        pass
    print("--------------- ç¯å¢ƒæ£€æŸ¥å®Œæ¯• ---------------")


# ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šæ‰“å°å®æ—¶çŠ¶æ€æ¡ (å¢åŠ å½“å‰æ´»åŠ¨æ˜¾ç¤º)
def print_status_prompt(username, memory_mgr, sentiment_engine):
    # 1. ä¸ªäººçŠ¶æ€
    user_state = memory_mgr.get_user_state_obj()

    # 2. å…¨å±€çŠ¶æ€
    global_state = sentiment_engine.get_global_state()
    mood = global_state["mood"]
    energy = global_state["energy"]

    # ğŸ”¥ æ–°å¢ï¼šè·å–å½“å‰æ´»åŠ¨ï¼Œå¹¶æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬ï¼Œè®©ä½ çŸ¥é“å¥¹åœ¨å¹²å˜›
    current_act = global_state.get("current_activity", "æœªçŸ¥")
    if len(current_act) > 10: current_act = current_act[:9] + "..."

    # å›¾æ ‡æ˜ å°„
    mood_icon = "ğŸ˜"
    if mood >= 80:
        mood_icon = "ğŸ˜†"
    elif mood >= 60:
        mood_icon = "ğŸ˜Š"
    elif mood <= 20:
        mood_icon = "ğŸ˜¡"
    elif mood <= 40:
        mood_icon = "ğŸ˜"

    energy_icon = "âš¡"
    if energy < 30: energy_icon = "ğŸª«"

    # æ„é€ çŠ¶æ€æ¡ (å¢åŠ  ğŸ“… æ´»åŠ¨æ˜¾ç¤º)
    status_bar = f"\n[ğŸ’– {int(user_state.affection)} | ğŸ“… {current_act} | {energy_icon} {int(energy)} | {mood_icon} {int(mood)}] ğŸ‘¤ {username}: "
    print(status_bar, end="", flush=True)


# ================= ğŸ’¾ å¼ºåˆ¶é€€å‡ºä¿æŠ¤é’©å­ =================
def emergency_save():
    if global_memory_mgr:
        print("\nğŸš¨ [ç´§æ€¥å­˜æ¡£] æ£€æµ‹åˆ°ç¨‹åºå¼‚å¸¸ä¸­æ–­ï¼Œæ­£åœ¨å°è¯•å¼ºåˆ¶ä¿å­˜...")
        global_memory_mgr.save()
        print("âœ… [ç´§æ€¥å­˜æ¡£] æ•°æ®å·²å†™å›ç£ç›˜ã€‚")


atexit.register(emergency_save)

# ================= ğŸ•°ï¸ åå°ç›‘è§†ä»»åŠ¡ =================
current_bg_file = ""


async def update_scene_logic(activity_text, vts):
    global current_bg_file

    target_bg = DEFAULT_BACKGROUND

    # éå†å…³é”®è¯å¯»æ‰¾åŒ¹é…çš„èƒŒæ™¯
    for keyword, filename in SCENE_MAP.items():
        if keyword in activity_text:
            target_bg = filename
            break

    if target_bg != current_bg_file:
        print(f"ğŸ–¼ï¸ [åœºæ™¯åˆ‡æ¢] æ£€æµ‹åˆ°æ´»åŠ¨ '{activity_text}' -> åˆ‡æ¢èƒŒæ™¯: {target_bg}")
        await vts.set_background(target_bg)
        current_bg_file = target_bg


# ================= ğŸ•°ï¸ åå°ç›‘è§†ä»»åŠ¡ (V22.0 å‰§æƒ…æ¨è¿›ç‰ˆ) =================
async def monitor_idle_status(vts, audio_mgr, brain, memory_mgr, sentiment_engine, bgm_mgr, input_mgr):
    global last_interaction_time, CURRENT_SPEAK_TASK

    IDLE_START_TIME = 30
    PROACTIVE_TALK_THRESHOLD = 120  # 2åˆ†é’Ÿä¸è¯´è¯è§¦å‘

    last_idle_action_time = 0
    has_triggered_talk = False
    idle_talk_sequence = 0

    last_detected_activity = sentiment_engine.get_global_state().get("current_activity", "")

    while True:
        await asyncio.sleep(1)

        if input_mgr.is_processing:
            last_interaction_time = time.time()
            continue

        user_state = memory_mgr.get_user_state_obj()
        g_state = sentiment_engine.get_global_state()
        current_energy = g_state["energy"]
        current_mood = g_state["mood"]
        current_act = g_state.get("current_activity", "")

        await update_scene_logic(current_act, vts)
        if bgm_mgr: bgm_mgr.update_state(current_mood, current_energy, current_act)

        if user_state.affection <= -50 or current_energy < 10:
            continue

        if CURRENT_SPEAK_TASK and not CURRENT_SPEAK_TASK.done():
            last_interaction_time = time.time()
            continue

        now = time.time()
        idle_duration = now - last_interaction_time

        if idle_duration < 2:
            idle_talk_sequence = 0
            has_triggered_talk = False
            last_detected_activity = current_act

        if idle_duration > IDLE_START_TIME and (now - last_idle_action_time) > 15:
            if current_energy > 20:
                safe_actions = list(ACTIONS.keys())
                if safe_actions:
                    await vts.trigger_action(random.choice(safe_actions))
            last_idle_action_time = now

        is_scene_changed = (current_act != last_detected_activity) and (current_act != "")

        if is_scene_changed or (idle_duration > PROACTIVE_TALK_THRESHOLD and not has_triggered_talk):
            if current_energy >= 30:
                idle_talk_sequence += 1

                injection = f"ã€ç³»ç»Ÿæç¤ºã€‘ç”¨æˆ·å·²ç»æ²‰é»˜äº† {int(idle_duration)} ç§’ã€‚"
                if is_scene_changed:
                    injection += f"\nâš ï¸ã€åœºæ™¯åˆ‡æ¢ã€‘ä½ çš„æ´»åŠ¨åˆšåˆšä»â€œ{last_detected_activity}â€å˜æˆäº†â€œ{current_act}â€ã€‚"
                    injection += "\nğŸ’¡ã€å›å¤æŒ‡å¼•ã€‘è¯·ç»“åˆâ€œç”¨æˆ·é•¿æ—¶é—´ä¸ç†ä½ â€è¿™ä¸ªäº‹å®ï¼Œè¡¨ç°å‡ºå‚²å¨‡æˆ–ä¸æ»¡ã€‚"
                    injection += "\nä¾‹å¦‚ï¼šâ€œå“¼ï¼Œæ—¢ç„¶ä½ åŠå¤©éƒ½ä¸è¯´è¯ï¼Œé‚£æœ¬èŠ™å®å¨œè¦å»ï¼ˆæ–°æ´»åŠ¨ï¼‰äº†ï¼Œä¸é™ªä½ å‘å‘†äº†ï¼â€"
                    print(f"ğŸ¬ [å‰§æƒ…æ¨è¿›] æ£€æµ‹åˆ°åœºæ™¯åˆ‡æ¢: {last_detected_activity} -> {current_act}")
                else:
                    print(f"ğŸ‘€ [è§‚å¯Ÿ] ç”¨æˆ·æ²‰é»˜ï¼Œå°è¯•ç¬¬ {idle_talk_sequence} æ¬¡æ­è¯...")

                reply = brain.think(
                    memory_mgr,
                    sentiment_injection=injection,
                    is_proactive=True,
                    proactive_stage=idle_talk_sequence
                )

                dirty_phrases = ["å–‚ï¼è®©æˆ‘æŠŠè¯è¯´å®Œï¼", "è®©æˆ‘æŠŠè¯è¯´å®Œ"]
                for phrase in dirty_phrases: reply = reply.replace(phrase, "")
                if "[" not in reply: reply = f"[å‚²å¨‡] {reply}"

                print(f"\nğŸ­ èŠ™å®å¨œ(ä¸»åŠ¨): {reply}")
                memory_mgr.add_history("assistant", reply)
                memory_mgr.save()
                CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(reply, vts))
                print_status_prompt(memory_mgr.current_user, memory_mgr, sentiment_engine)

            last_interaction_time = time.time()
            has_triggered_talk = True
            last_detected_activity = current_act


# ================= ğŸ§ å¼‚æ­¥ç›‘å¬å¾ªç¯ =================
async def listen_loop(input_mgr, username):
    print("ğŸ¤ [ç³»ç»Ÿ] ç›‘å¬æœåŠ¡å·²å¯åŠ¨ (è¾“å…¥ 'exit' é€€å‡º)...")
    while True:
        try:
            text = await aioconsole.ainput("")
            input_mgr.add_message(text)
        except asyncio.CancelledError:
            break
        except Exception as e:
            await asyncio.sleep(1)


# ================= ğŸ¬ ä¸»ç¨‹åº =================
async def main():
    global CURRENT_SPEAK_TASK, last_interaction_time, global_memory_mgr

    print("\nğŸ“š === èŠ™å®å¨œçš„è®°å¿†æ®¿å ‚ ===")
    username = input("è¯·è¾“å…¥ä½ çš„åå­— (è¯»å–å­˜æ¡£): ").strip()
    if not username: username = "æ—…è¡Œè€…"

    launch_services()

    vts = VTSController(port=VTS_PORT)
    audio_mgr = AudioManager()
    bgm_mgr = BGMManager()
    brain = Brain()
    memory_mgr = MemoryManager()
    global_memory_mgr = memory_mgr
    sentiment_engine = SentimentEngine()
    input_mgr = InputBufferManager(timeout=INPUT_TIMEOUT)

    memory_mgr.load_user(username)
    user_state = memory_mgr.get_user_state_obj()
    g_state = sentiment_engine.get_global_state()

    if not await vts.connect(): return

    lvl, title, _ = memory_mgr.calculate_status()
    print(f"\nğŸ‰ === èŠ™å®å¨œ & {username} ===")
    print(f"ğŸ’– å¥½æ„Ÿåº¦: {user_state.affection} | ğŸ­ ç­‰çº§: {title}")
    print(f"âš¡ ç²¾åŠ›å€¼: {int(g_state['energy'])}/100 | ğŸ˜Š å¿ƒæƒ…: {int(g_state['mood'])}/100")
    print("==============================\n")

    chat_history = memory_mgr.data.get('chat_history', [])
    summary = memory_mgr.data.get("summary", "")
    is_new_user = (len(chat_history) == 0) and (not summary)

    welcome = ""

    if is_new_user:
        welcome = f"[å‚²å¨‡] å’³å’³ï¼åˆæ¬¡è§é¢ï¼æˆ‘æ˜¯èŠ™å®å¨œÂ·å¾·Â·æ«ä¸¹ï¼"
    elif user_state.affection <= -100:
        welcome = "[ç”Ÿæ°”] â€¦â€¦ï¼ˆæ— è§†ï¼‰"
    else:
        welcome = f"[å‚²å¨‡] åˆæ˜¯ä½ å•Šï¼Œ{username}ã€‚"
        if user_state.affection >= 300:
            print("ğŸ¤” (èŠ™èŠ™æ­£åœ¨å›å¿†ä¸Šæ¬¡èŠäº†ä»€ä¹ˆ...)")
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ get å®‰å…¨ä¼ å…¥å½“å‰æ´»åŠ¨å’Œåœ°ç‚¹
            dynamic_welcome = brain.generate_dynamic_welcome(
                memory_mgr,
                g_state['mood'],
                g_state['energy'],
                g_state.get('current_activity', 'å‘å‘†'),
                g_state.get('current_location', 'å®¶é‡Œ')
            )
            if dynamic_welcome:
                welcome = dynamic_welcome
            else:
                welcome = f"[ç¬‘] {username}ï¼ä½ ç»ˆäºæ¥äº†ï¼Œæˆ‘ç­‰ä½ å¥½ä¹…äº†ï¼"

    print(f"ğŸ­ èŠ™å®å¨œ: {welcome}")
    if user_state.affection > -100:
        memory_mgr.add_history("assistant", welcome)

    import datetime
    now_h = datetime.datetime.now().hour
    if 22 <= now_h or now_h < 7:
        await vts.trigger_action("å›°")
    else:
        await vts.trigger_action("æ‘Šæ‰‹")

    CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(welcome, vts))

    monitor_task = asyncio.create_task(
        monitor_idle_status(vts, audio_mgr, brain, memory_mgr, sentiment_engine, bgm_mgr, input_mgr))
    listen_task = asyncio.create_task(listen_loop(input_mgr, username))

    print_status_prompt(username, memory_mgr, sentiment_engine)

    try:
        while True:
            await asyncio.sleep(0.1)
            memory_mgr.data["last_interaction_timestamp"] = time.time()

            g_state = sentiment_engine.get_global_state()
            current_act_text = g_state.get("current_activity", "")
            if bgm_mgr:
                bgm_mgr.update_state(g_state['mood'], g_state['energy'], current_act_text)

            if input_mgr.has_finished_speaking():
                user_input = input_mgr.pop_full_message()

                if user_input.lower() in ["quit", "exit", "é€€å‡º", "å†è§", "æ‹œæ‹œ"]:
                    print("ğŸ’¾ ç”¨æˆ·è¯·æ±‚é€€å‡º...")
                    break

                input_mgr.is_processing = True
                last_interaction_time = time.time()
                print(f"\nğŸ“ [ç”¨æˆ·] {user_input}")

                current_user_state = memory_mgr.get_user_state_obj()
                new_user_state, prompt_injection, instant_action, reply_override = sentiment_engine.analyze(user_input,
                                                                                                            current_user_state)
                memory_mgr.save_user_state(new_user_state)

                is_stop, bl_reply, bl_action = sentiment_engine.check_blacklist_state(new_user_state)
                if is_stop:
                    if bl_reply:
                        CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(bl_reply, vts))
                    elif bl_action:
                        await vts.trigger_action(bl_action)
                    input_mgr.is_processing = False
                    print_status_prompt(username, memory_mgr, sentiment_engine)
                    continue

                if instant_action:
                    print(f"âš¡ [åŠ¨ä½œè§¦å‘] {instant_action}")
                    await vts.trigger_action(instant_action)

                is_interrupting = False
                ignore_interrupt_list = ["å“ˆå“ˆ", "å—¯", "å—¯å—¯", "å¯¹", "æ˜¯", "å“¦", "å•Š", "tql", "666", "ç¡®å®", "å¥½",
                                         "ç»§ç»­"]
                is_short_reply = len(user_input) < 4 or any(w in user_input for w in ignore_interrupt_list)

                if audio_mgr.voice_channel.get_busy() and not is_short_reply:
                    is_interrupting = True
                    audio_mgr.stop()
                elif CURRENT_SPEAK_TASK and not CURRENT_SPEAK_TASK.done() and not is_short_reply:
                    CURRENT_SPEAK_TASK.cancel()

                if is_short_reply and audio_mgr.voice_channel.get_busy():
                    print("ğŸ”‡ [ç³»ç»Ÿ] æ£€æµ‹åˆ°çŸ­å›å¤ï¼Œä¸æ‰“æ–­èŠ™èŠ™è¯´è¯ã€‚")

                if reply_override:
                    final_text = reply_override
                else:
                    print("â³ (å¤§è„‘æ€è€ƒä¸­...)")
                    final_text = brain.think(memory_mgr, user_input, sentiment_injection=prompt_injection)

                if is_interrupting:
                    int_score, int_msg, int_phrase = sentiment_engine.get_interruption_reaction()
                    memory_mgr.update_affection(int_score)
                    print(f"\nğŸ’¢ è§¦å‘æ‰“æ–­: {int_phrase}")
                    final_text = f"{int_phrase} {final_text}"
                    audio_mgr.stop()
                    try:
                        await CURRENT_SPEAK_TASK
                    except:
                        pass

                memory_mgr.add_history("assistant", final_text)
                memory_mgr.save()
                print(f"\rğŸ­ èŠ™å®å¨œ: {final_text}")

                CURRENT_SPEAK_TASK = asyncio.create_task(audio_mgr.speak(final_text, vts))
                asyncio.create_task(memory_mgr.compress_memory_if_needed(brain))

                last_interaction_time = time.time()
                input_mgr.is_processing = False
                print_status_prompt(username, memory_mgr, sentiment_engine)

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ [ç³»ç»Ÿ] æ£€æµ‹åˆ°å¼ºåˆ¶é€€å‡ºä¿¡å·...")

    finally:
        print("\nğŸ“ æ­£åœ¨è¿›è¡Œæ•£åœºæ•´ç† (å½’æ¡£è®°å¿†ä¸­)...")
        if 'monitor_task' in locals() and not monitor_task.done():
            monitor_task.cancel()
        if 'listen_task' in locals() and not listen_task.done():
            listen_task.cancel()
        try:
            if hasattr(memory_mgr, "archive_session"):
                memory_mgr.archive_session(brain)
            else:
                memory_mgr.save()
        except Exception as e:
            print(f"âš ï¸ å½’æ¡£è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        if vts:
            await vts.close()
        print("ğŸ‘‹ èŠ™å®å¨œ: ä¸‹æ¬¡æ¼”å‡ºå†è§å•¦ï¼(ç¨‹åºå·²å…³é—­)")


if __name__ == "__main__":
    asyncio.run(main())